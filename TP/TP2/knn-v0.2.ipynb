{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\">TP2: Primer KNN eliminando filas con columnas nulas y regresion</h1>\n",
    "![](../data/icon_properati-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# modules\n",
    "import knn as knnlibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "properties =knnlibrary.get_dataset()\n",
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transformo el campo fecha\n",
    "properties_caba = knnlibrary.transform_date(properties)\n",
    "\n",
    "properties_caba.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filtro por CABA y GBA\n",
    "# queremos solo las propiedades que tienen precio y eliminamos columnas que sabemos que no son \n",
    "#redundantes y que no nos servirian para knn\n",
    "# eliminamos propiedades con mas de 54 pisos\n",
    "properties_caba = knnlibrary.clean_dataset(properties)\n",
    "\n",
    "# las expensas tienen demasiados nulos por lo que voy a eliminar esa columna\n",
    "properties_caba = properties_caba.drop(['expenses'], axis = 1)\n",
    "\n",
    "properties_caba.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# eliminamos filas con valores nulo\n",
    "properties_caba = properties_caba.dropna(how='any')\n",
    "properties_caba.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# atributos categoricos\n",
    "encoder = LabelEncoder()\n",
    "properties_caba = knnlibrary.encoder_attributes(properties_caba, encoder)\n",
    "\n",
    "properties_caba.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separacion del set de train para Cross-validation, Normalizacion y feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# separamos el train de traing para validarlo luego usando un 30% de los datos\n",
    "X, y = properties_caba.iloc[:, properties_caba.columns != 'price'].values, properties_caba.iloc[:, properties_caba.columns == 'price'].values\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=now.microsecond)\n",
    "    \n",
    "len(X_test), len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a cada dato le restamos la media y lo dividimos por su desviacion standard\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)\n",
    "\n",
    "# en la documentacion de sklearn menciona sobre la regularizacion pero la voy a escapear por ahora\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression(penalty='l1')\n",
    "\n",
    "logi_regr = LogisticRegression(penalty='l1', C=0.1)\n",
    "logi_regr.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', logi_regr.score(X_train_std, y_train))\n",
    "print('Test accuracy:', logi_regr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca vemos que hay algo extrano porque no hicimos ningun tipo de moficacion en los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, median_absolute_error, mean_squared_error\n",
    "\n",
    "# clase para hacer Backward Selection\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features,\n",
    "        scoring=accuracy_score,\n",
    "        test_size=0.25, random_state=1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=self.test_size,\n",
    "        random_state=self.random_state)\n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train,\n",
    "        X_test, y_test, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "            for p in combinations(self.indices_, r=dim-1):\n",
    "                score = self._calc_score(X_train, y_train,\n",
    "                X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "        \n",
    "    def _calc_score(self, X_train, y_train,\n",
    "                        X_test, y_test, indices):\n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score\n",
    "\n",
    "# aplicamos algoritmos\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "sbs = SBS(knn, k_features=1, scoring=mean_squared_error)\n",
    "sbs.fit(X_train_std, y_train)\n",
    "#y_pred = knn.fit(X_train_std, y_train).predict(X_test_std)\n",
    "#median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sbs.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sbs.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graficamos\n",
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "plot(k_feat, sbs.scores_, marker='o')\n",
    "#ylim([0.0, 1.1])\n",
    "ylabel('Mean Squared Error')\n",
    "xlabel('Number of features')\n",
    "grid()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hacemos una prueba en el set original\n",
    "knn.fit(X_train_std, y_train)\n",
    "print('Training accuracy:', knn.score(X_train_std, y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_param = list(sbs.subsets_[0])\n",
    "print(properties_caba.iloc[:, properties_caba.columns != 'price'].columns[k_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y como clasifica tomando solo los features devueltos\n",
    "knn.fit(X_train_std[:, k_param], y_train)\n",
    "print('Training accuracy:', knn.score(X_train_std[:, k_param], y_train))\n",
    "print('Test accuracy:', knn.score(X_test_std[:, k_param], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creo que luego de varias corridas el mejor resultado se da cuando seleccionamos todos los features. Por lo que nos quedamos con este resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccion de la cantidad optima de vecinos y la metrica a utilizar\n",
    "Elegimos k usando grid search y luego p usando random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_scores = []\n",
    "k_list = []\n",
    "p_list = []\n",
    "labels = []\n",
    "\n",
    "best_score = 0\n",
    "best_k = 0\n",
    "best_p = 0\n",
    "\n",
    "for i in range(5,14):\n",
    "    for p in np.random.randint(2,10,size=4):\n",
    "        knn = KNeighborsRegressor(n_neighbors=i, weights='distance', p=p)\n",
    "        knn.fit(X_train_std[:, k_param], y_train)\n",
    "\n",
    "        p_list.append(p)\n",
    "        k_list.append(i)\n",
    "        \n",
    "        sc = knn.score(X_test_std[:, k_param], y_test)\n",
    "        \n",
    "        labels.append(str(round(sc,3)))\n",
    "        k_scores.append(sc)\n",
    "        \n",
    "        if(sc > best_score):\n",
    "            best_score = sc\n",
    "            best_k = i\n",
    "            best_p = p\n",
    "\n",
    "mu = np.mean(k_scores)\n",
    "v = np.var(k_scores)\n",
    "\n",
    "k_scores = list(map(lambda x: ((x-mu)/v)*50, k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graficamos\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(k_list, p_list, s=k_scores, alpha=0.5)\n",
    "\n",
    "title = \"K vs Distancia - best k=%s, best p=%s, score=%s\"%(best_k, best_p, best_score)\n",
    "plt.title(title)\n",
    "ylabel('Distance')\n",
    "xlabel('k values')\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    ax.annotate(label, (k_list[i],p_list[i]))\n",
    "\n",
    "ax.grid(linestyle='-', linewidth='0.5', color='red')\n",
    "\n",
    "plt.savefig('/home/jovyan/work/TP/TP2/img/best_k=%s-best_p=%s-score=%s.png'%(best_k, best_p, best_score));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corridas con p=2\n",
    "Primera corrida: p=2 k=5\n",
    "</br>Corridas posteriores: p=2, k entre 9 y 13\n",
    "\n",
    "### Corridas modificando tanto p como k\n",
    "<ul>\n",
    "<li>p=46, k=3, score=0.93</li>\n",
    "<li>p=57, k=3, score=0.933</li>\n",
    "<li>p=4, k=2, score=0.905</li>\n",
    "<li>p=82, k=4, score=0.89477</li>\n",
    "<li>p=81, k=5, score=0.8630</li>\n",
    "<li>p=2, k=6, score=0.9638</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# leemos set de test\n",
    "test_df = pd.read_csv('../data/test/properati_dataset_testing_noprice.csv', low_memory=False)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transformamos atributos categoricos\n",
    "test_df['place_name'] = encoder.fit_transform(test_df[['place_name']])\n",
    "test_df['state_name'] = encoder.fit_transform(test_df[['state_name']])\n",
    "test_df['place_with_parent_names'] = encoder.fit_transform(test_df[['place_with_parent_names']])\n",
    "test_df['property_type'] = encoder.fit_transform(test_df[['property_type']])\n",
    "\n",
    "# tranformamos fechas\n",
    "X_test_df = knnlibrary.transform_date(test_df)\n",
    "X_test_df = X_test_df[['floor', 'lat', 'lon', 'place_name', 'place_with_parent_names',\n",
    "       'property_type', 'rooms', 'state_name', 'surface_covered_in_m2',\n",
    "       'surface_total_in_m2', 'created_on_year', 'created_on_month',\n",
    "       'created_on_day']]\n",
    "\n",
    "# completamos valores nan\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer_mean = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "X_test_df['floor'] = X_test_df[['floor']].fillna(1)\n",
    "X_test_df['rooms'] = X_test_df[['rooms']].fillna(1)\n",
    "\n",
    "X_test_df[\"surface_total_in_m2\"] = imputer_mean.fit_transform(X_test_df[[\"surface_total_in_m2\"]])\n",
    "X_test_df[\"surface_covered_in_m2\"] = imputer_mean.fit_transform(X_test_df[[\"surface_covered_in_m2\"]])\n",
    "X_test_df[\"lat\"] = imputer_mean.fit_transform(X_test_df[[\"lat\"]])\n",
    "X_test_df[\"lon\"] = imputer_mean.fit_transform(X_test_df[[\"lon\"]])\n",
    "\n",
    "X_test_std_df = stdsc.transform(X_test_df)\n",
    "\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_test_std_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=best_k, weights='distance', p=best_p)\n",
    "knn.fit(X_train_std[:, k_param], y_train)\n",
    "X_test_df['prediction'] = knn.predict(X_test_std_df)\n",
    "X_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"id\":test_df[\"id\"], \"price_usd\":X_test_df['prediction']} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ejecutamos knn\n",
    "output.to_csv( \"../data/result/result_\"+str(now)+\".csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
